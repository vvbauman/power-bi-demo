{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion & Brief EDA notebook\n",
    "This notebook is for incremental development of modularized classes for ingesting the various datasets in the loan application dataset\n",
    "\n",
    "The goals for data ingestion in src are to\n",
    "1) Ingest all data sources (bronze tables) as individual pandas dataframes\n",
    "2) Merge all datasets into one large dataframe suitable for analysis (one silver table)\n",
    "\n",
    "Keep in mind that overall project goal is to predict the loan outcome for finished loans at the time of loan start\n",
    "\n",
    "## EDA\n",
    "Start developing data ingestion and merging code. Do not prioritize modularizing the code. Investigation OK here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import pandera as pa\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# url to raw content in repo\n",
    "data_url= 'https://raw.githubusercontent.com/vvbauman/sample-work-loan-application/feature/data-ingestion/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest all data sources as pandas dataframes - these are saved as bronze tables\n",
    "account= pd.read_csv(data_url + 'account.txt', sep= ';')\n",
    "card= pd.read_csv(data_url + 'card.txt', sep= ';')\n",
    "client= pd.read_csv(data_url + 'client.txt', sep= ';')\n",
    "disp= pd.read_csv(data_url + 'disp.txt',  sep= ';')\n",
    "district= pd.read_csv(data_url + 'district.txt',  sep= ';')\n",
    "loan= pd.read_csv(data_url + 'loan.txt', sep= ';')\n",
    "order= pd.read_csv(data_url + 'order.txt', sep= ';')\n",
    "transactions= pd.read_csv(data_url + 'trans.txt', sep= ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n",
      "No null values in dataframe. Returned dataframe is same as input dataframe\n"
     ]
    }
   ],
   "source": [
    "# check for null values. If less than 5% of overall data, drop rows with nulls\n",
    "# option to provide list subset, used in pd.dropna() to only consider a subset of rows when counting null values\n",
    "def drop_nulls(df : pd.DataFrame, subset : list = []):\n",
    "    if len(subset) == 0:\n",
    "        percent_null= df.isnull().sum().sum() / len(df)\n",
    "    else:\n",
    "        percent_null= df[subset].isnull().sum().sum() / len(df)\n",
    "\n",
    "    if percent_null == 0:\n",
    "        print('No null values in dataframe. Returned dataframe is same as input dataframe')\n",
    "        return df\n",
    "    elif percent_null < 0.05:\n",
    "        print('Less than 5% of rows are missing data. Returned dataframe is same as input dataframe with these rows dropped')\n",
    "        return df.dropna(subset= subset)\n",
    "    else:\n",
    "        print('More than 5% of rows are missing data. Returning dataframe without dropping rows')\n",
    "        return df\n",
    "\n",
    "account= drop_nulls(account)\n",
    "card= drop_nulls(card)\n",
    "client= drop_nulls(client)\n",
    "disp= drop_nulls(disp)\n",
    "district= drop_nulls(district)\n",
    "loan= drop_nulls(loan)\n",
    "order= drop_nulls(order)\n",
    "transactions= drop_nulls(transactions, subset= ['trans_id', 'account_id', 'date'])\n",
    "\n",
    "# if any rows are dropped, this would be a good place to save these tables as silver tables, since they have undergone some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4500 entries, 0 to 4499\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   account_id   4500 non-null   int64 \n",
      " 1   district_id  4500 non-null   int64 \n",
      " 2   frequency    4500 non-null   object\n",
      " 3   date         4500 non-null   int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 140.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 892 entries, 0 to 891\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   card_id  892 non-null    int64 \n",
      " 1   disp_id  892 non-null    int64 \n",
      " 2   type     892 non-null    object\n",
      " 3   issued   892 non-null    object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 28.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5369 entries, 0 to 5368\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   client_id     5369 non-null   int64\n",
      " 1   birth_number  5369 non-null   int64\n",
      " 2   district_id   5369 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 126.0 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5369 entries, 0 to 5368\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   disp_id     5369 non-null   int64 \n",
      " 1   client_id   5369 non-null   int64 \n",
      " 2   account_id  5369 non-null   int64 \n",
      " 3   type        5369 non-null   object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 167.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77 entries, 0 to 76\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A1      77 non-null     int64  \n",
      " 1   A2      77 non-null     object \n",
      " 2   A3      77 non-null     object \n",
      " 3   A4      77 non-null     int64  \n",
      " 4   A5      77 non-null     int64  \n",
      " 5   A6      77 non-null     int64  \n",
      " 6   A7      77 non-null     int64  \n",
      " 7   A8      77 non-null     int64  \n",
      " 8   A9      77 non-null     int64  \n",
      " 9   A10     77 non-null     float64\n",
      " 10  A11     77 non-null     int64  \n",
      " 11  A12     77 non-null     object \n",
      " 12  A13     77 non-null     float64\n",
      " 13  A14     77 non-null     int64  \n",
      " 14  A15     77 non-null     object \n",
      " 15  A16     77 non-null     int64  \n",
      "dtypes: float64(2), int64(10), object(4)\n",
      "memory usage: 9.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 682 entries, 0 to 681\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   loan_id     682 non-null    int64  \n",
      " 1   account_id  682 non-null    int64  \n",
      " 2   date        682 non-null    int64  \n",
      " 3   amount      682 non-null    int64  \n",
      " 4   duration    682 non-null    int64  \n",
      " 5   payments    682 non-null    float64\n",
      " 6   status      682 non-null    object \n",
      "dtypes: float64(1), int64(5), object(1)\n",
      "memory usage: 37.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6471 entries, 0 to 6470\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   order_id    6471 non-null   int64  \n",
      " 1   account_id  6471 non-null   int64  \n",
      " 2   bank_to     6471 non-null   object \n",
      " 3   account_to  6471 non-null   int64  \n",
      " 4   amount      6471 non-null   float64\n",
      " 5   k_symbol    6471 non-null   object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 303.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 648460 entries, 0 to 648459\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   trans_id    648460 non-null  int64  \n",
      " 1   account_id  648460 non-null  int64  \n",
      " 2   date        648460 non-null  int64  \n",
      " 3   type        648460 non-null  object \n",
      " 4   operation   537540 non-null  object \n",
      " 5   amount      648460 non-null  float64\n",
      " 6   balance     648460 non-null  float64\n",
      " 7   k_symbol    344908 non-null  object \n",
      " 8   bank        165156 non-null  object \n",
      " 9   account     176675 non-null  float64\n",
      "dtypes: float64(3), int64(3), object(4)\n",
      "memory usage: 49.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get info for all dataframes, to understand how the star schema can be configured\n",
    "for i in [account, card, client, disp, district, loan, order, transactions]:\n",
    "    print(i.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'account_id'}\n",
      "{'client_id'}\n"
     ]
    }
   ],
   "source": [
    "# take advantage of the fact there are common column names across dataframes and assume they mean the same thing across dataframes\n",
    "# (i.e., will be able to join on these columns and there will be common elements within these columns across the multiple tables)\n",
    "\n",
    "# confirm that the common column among account, loan, order, transactions, and disp is \"account_id\"\n",
    "print(set(account.columns) & set(loan.columns) & set(order.columns) & set(transactions.columns) & set(disp.columns))\n",
    "\n",
    "# confirm that the common column among disp and client is \"client_id\"\n",
    "print(set(disp.columns) & set(client.columns))\n",
    "\n",
    "# card can be joined with disp - disp has \"account_id\" and \"disp_id columns\"\n",
    "# no common columns in the district dataframe with any of the other dataframes. Do merges for above dataframes, then see how district fits in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 330720 entries, 0 to 330719\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   account_id             330720 non-null  int64  \n",
      " 1   district_id            330720 non-null  int64  \n",
      " 2   frequency              330720 non-null  object \n",
      " 3   date_account           330720 non-null  int64  \n",
      " 4   loan_id                330720 non-null  int64  \n",
      " 5   date_loan              330720 non-null  int64  \n",
      " 6   amount_loan            330720 non-null  int64  \n",
      " 7   duration               330720 non-null  int64  \n",
      " 8   payments               330720 non-null  float64\n",
      " 9   status                 330720 non-null  object \n",
      " 10  order_id               330720 non-null  int64  \n",
      " 11  bank_to                330720 non-null  object \n",
      " 12  account_to             330720 non-null  int64  \n",
      " 13  amount_order           330720 non-null  float64\n",
      " 14  k_symbol_order         330720 non-null  object \n",
      " 15  trans_id               330720 non-null  int64  \n",
      " 16  date                   330720 non-null  int64  \n",
      " 17  type_transactions      330720 non-null  object \n",
      " 18  operation              280293 non-null  object \n",
      " 19  amount                 330720 non-null  float64\n",
      " 20  balance                330720 non-null  float64\n",
      " 21  k_symbol_transactions  178418 non-null  object \n",
      " 22  bank                   103489 non-null  object \n",
      " 23  account                123304 non-null  float64\n",
      " 24  disp_id                330720 non-null  int64  \n",
      " 25  client_id              330720 non-null  int64  \n",
      " 26  type_disp              330720 non-null  object \n",
      "dtypes: float64(5), int64(13), object(9)\n",
      "memory usage: 70.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# inner-join account, loan, order, transactions, and disp dataframes on account_id\n",
    "# we only want details on accounts with a loan\n",
    "account_id_merge= (account\n",
    "                    .merge(loan, on= ['account_id'], how= 'inner', suffixes= ('_account', '_loan')) \n",
    "                    .merge(order, on= ['account_id'], how= 'inner', suffixes= ('_loan', '_order'))\n",
    "                    .merge(transactions, on= ['account_id'], how= 'inner', suffixes= ('_order', '_transactions'))\n",
    "                    .merge(disp, on= ['account_id'], how= 'inner', suffixes= ('_transactions', '_disp'))\n",
    "                     )\n",
    "account_id_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 75599 entries, 0 to 75598\n",
      "Data columns (total 35 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   disp_id                75599 non-null  int64  \n",
      " 1   client_id              75599 non-null  int64  \n",
      " 2   account_id             75599 non-null  int64  \n",
      " 3   type_client_disp       75599 non-null  object \n",
      " 4   birth_number           75599 non-null  int64  \n",
      " 5   district_id            75599 non-null  int64  \n",
      " 6   card_id                75599 non-null  int64  \n",
      " 7   type_card              75599 non-null  object \n",
      " 8   issued                 75599 non-null  object \n",
      " 9   district_id_disp_id    75599 non-null  int64  \n",
      " 10  frequency              75599 non-null  object \n",
      " 11  date_account           75599 non-null  int64  \n",
      " 12  loan_id                75599 non-null  int64  \n",
      " 13  date_loan              75599 non-null  int64  \n",
      " 14  amount_loan            75599 non-null  int64  \n",
      " 15  duration               75599 non-null  int64  \n",
      " 16  payments               75599 non-null  float64\n",
      " 17  status                 75599 non-null  object \n",
      " 18  order_id               75599 non-null  int64  \n",
      " 19  bank_to                75599 non-null  object \n",
      " 20  account_to             75599 non-null  int64  \n",
      " 21  amount_order           75599 non-null  float64\n",
      " 22  k_symbol_order         75599 non-null  object \n",
      " 23  trans_id               75599 non-null  int64  \n",
      " 24  date                   75599 non-null  int64  \n",
      " 25  type_transactions      75599 non-null  object \n",
      " 26  operation              64725 non-null  object \n",
      " 27  amount                 75599 non-null  float64\n",
      " 28  balance                75599 non-null  float64\n",
      " 29  k_symbol_transactions  38461 non-null  object \n",
      " 30  bank                   22336 non-null  object \n",
      " 31  account                27542 non-null  float64\n",
      " 32  disp_id_disp_id        75599 non-null  int64  \n",
      " 33  client_id_disp_id      75599 non-null  int64  \n",
      " 34  type_disp              75599 non-null  object \n",
      "dtypes: float64(5), int64(18), object(12)\n",
      "memory usage: 20.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# inner-join disp and client dataframes on client_id, then merge with card on disp_id, then merge with account_id_merge on disp_id\n",
    "client_id_merge= disp.merge(client, on= ['client_id'], how= 'inner', suffixes= ('_disp', '_client'))\n",
    "disp_id_merge= client_id_merge.merge(card, on= ['disp_id'], how= 'inner', suffixes= ('_client_disp', '_card'))\n",
    "\n",
    "silver= disp_id_merge.merge(account_id_merge, on= ['account_id'], how= 'inner', suffixes= ('', '_disp_id'))\n",
    "silver.info() # silver dataframe with 7/8 tables merged has 75599 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code modularization\n",
    "Create custom ingestion class that accomplishes all tasks done in cells above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_merge_schema= pa.DataFrameSchema({\n",
    "    'disp_id' : pa.Column(object, nullable= True),\n",
    "    'client_id' : pa.Column(object, nullable= True),\n",
    "    'account_id' : pa.Column(object, nullable= True),\n",
    "    'type_client_disp' : pa.Column(object, nullable= True),\n",
    "    'birth_number' : pa.Column('datetime64[ns]', nullable= True),\n",
    "    'district_id' : pa.Column(object, nullable= True),\n",
    "    'card_id' : pa.Column(object, nullable= True),\n",
    "    'type_card' : pa.Column(object, nullable= True),\n",
    "    'issued' : pa.Column('datetime64[ns]', nullable= True),\n",
    "    'district_id_disp_id' : pa.Column(object, nullable= True),\n",
    "    'frequency' : pa.Column(object, nullable= True),\n",
    "    'date_account' : pa.Column('datetime64[ns]', nullable= True),\n",
    "    'loan_id' : pa.Column(object, nullable= True),\n",
    "    'date_loan' : pa.Column('datetime64[ns]', nullable= True),\n",
    "    'amount_loan' : pa.Column(int, nullable= True),\n",
    "    'duration' : pa.Column(int, nullable= True),\n",
    "    'payments' : pa.Column(float, nullable= True),\n",
    "    'status' : pa.Column(object, nullable= True),\n",
    "    'order_id' : pa.Column(object, nullable= True),\n",
    "    'bank_to' : pa.Column(object, nullable= True),\n",
    "    'account_to' : pa.Column(object, nullable= True),\n",
    "    'amount_order' : pa.Column(float, nullable= True),\n",
    "    'k_symbol_order' : pa.Column(object, nullable= True),\n",
    "    'trans_id' : pa.Column(object, nullable= True),\n",
    "    'date' : pa.Column('datetime64[ns]', nullable= True),\n",
    "    'type_transactions' : pa.Column(object, nullable= True),\n",
    "    'operation' : pa.Column(object, nullable= True),\n",
    "    'amount' : pa.Column(float, nullable= True),\n",
    "    'balance' : pa.Column(float, nullable= True),\n",
    "    'k_symbol_transactions' : pa.Column(object, nullable= True),\n",
    "    'bank' : pa.Column(object, nullable= True),\n",
    "    'account' : pa.Column(object, nullable= True),\n",
    "    'disp_id_disp_id' : pa.Column(object, nullable= True),\n",
    "    'client_id_disp_id' : pa.Column(object, nullable= True),\n",
    "    'type_disp' : pa.Column(object, nullable= True)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(df : pd.DataFrame, schema : pa.DataFrameSchema, cols : list = []) -> str:\n",
    "    \"\"\" \n",
    "    Validate pandas dataframe schema\n",
    "    First, run schema check using panderas. If schema check fails, check that columns of interest (specified in cols argument) are at least present. \n",
    "    If these columns are not present, throw an error. If they are present, return the dataframe. Regardless of outcome, return a message \n",
    "    (message intended to display in print statement or in a logger)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe you want to validate the schema of\n",
    "    schema : expected/desired schema of df\n",
    "    cols : subset of columns in df you require the dataframe to have\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    msg : str describing results of schema check\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema(df, lazy= True)\n",
    "        msg= 'Correct schema, no action needed'\n",
    "    except:\n",
    "        if (len(cols) > 0) & (not set(df.columns) >= set(cols)):\n",
    "            msg= 'Provided schema does not match dataframe schema. Dataframe also does not have expected columns'\n",
    "        elif (len(cols) == 0) & (not set(df.columns) >= set(cols)):\n",
    "            msg= 'Provided schema does not match dataframe schema. Did not check for subset of columns - no argument provided to cols argument'\n",
    "        elif (len(cols) > 0) & (set(df.columns) >= set(cols)):\n",
    "            msg= 'Provided schema does not match dataframe schema but dataframe has expected columns'\n",
    "    return msg\n",
    "    \n",
    "def ingest_flat_file(path : str, filename : str, schema : pa.DataFrameSchema, sep : str = ','):\n",
    "    \"\"\"\n",
    "    Ingest a flat file saved to path and validate schema of ingested dataframe\n",
    "    Acceptable file formats are .csv, .txt, and .pkl. Any other provided formats will throw an error\n",
    "    Schema of ingested dataframe must exactly match the input argument schema otherwise an error will be thrown\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : absolute path to flat file, not including the flat file name and extension itself\n",
    "    filename : name and extension of flat file to be ingested\n",
    "    schema : expected schema of the ingested file\n",
    "    sep : separator to use in pd.read_csv() when attempting to ingest the flat file\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : ingested data\n",
    "    msg : str describing the results of data ingestion and schema check. Intended to display in print statement or log\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df= pd.read_csv(path + filename, sep= sep)\n",
    "    except: \n",
    "        try: \n",
    "            df= pd.read_pickle(path + filename)\n",
    "        except:\n",
    "            raise(Exception)\n",
    "\n",
    "    try:\n",
    "        validate_schema(df= df, schema= schema)\n",
    "        msg= 'Dataframe successfully ingested and has expected schema'\n",
    "    except:\n",
    "        msg= 'Dataframe successfully ingested but does not have expected schema'\n",
    "\n",
    "    return df, msg\n",
    "\n",
    "def save_flat_file(path : str, filename : str, df : pd.DataFrame) -> str:\n",
    "    \"\"\" \n",
    "    Save a pandas dataframe as a flat file to a specified location\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : absolute path to where the flat file should be saved. Do not include name or extension to save table as\n",
    "    filename : name and extension to save dataframe under \n",
    "    df : pandas dataframe to save as a flat file. This function does not include any data validation, so it should be conducted before running this function\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    msg : str describing the results of writing the dataframe. Intended to display in print statement or log\n",
    "    \"\"\"\n",
    "    try: \n",
    "        df.to_csv(path + filename)\n",
    "        msg= f'Dataframe successfully written to {path} (filename: {filename})'\n",
    "    except:\n",
    "        msg= f'Dataframe write unsuccessful. Check provided path and filename are correct (path: {path}, filename: {filename})'\n",
    "    return msg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nulls(df : pd.DataFrame, subset : list = [], logger : logging.Logger = None) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Function to drop rows in a dataframe containing null values, if the % of rows with null values is less than 5% of the entire dataframe length\n",
    "    If logging.Logger object provided to input arguments, log messages on rows being dropped will be written to logger\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe to check for null values\n",
    "    subset : list of str. Columns in df to consider when checking for null values. \n",
    "            If this argument is provided, any columns not in this list are not considered when checking for nulls\n",
    "    logger : logging.Logger object to write log messages to\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : same as input df but with rows with null values dropped (as long as they account for less than 5% of the data)\n",
    "    \"\"\"\n",
    "    if len(subset) == 0:\n",
    "        percent_null= df.isnull().sum().sum() / len(df)\n",
    "    else:\n",
    "        percent_null= df[subset].isnull().sum().sum() / len(df)\n",
    "\n",
    "    if percent_null == 0:\n",
    "        if isinstance(logger, logging.Logger):\n",
    "            logger.info('No null values in dataframe. Returned dataframe is same as input dataframe')\n",
    "        return df\n",
    "    elif percent_null < 0.05:\n",
    "        if isinstance(logger, logging.Logger):\n",
    "            logger.warning('Less than 5% of rows are missing data. Returned dataframe is same as input dataframe with these rows dropped')\n",
    "        return df.dropna(subset= subset)\n",
    "    else:\n",
    "        if isinstance(logger, logging.Logger):\n",
    "            logger.warning('More than 5% of rows are missing data. Returning dataframe without dropping rows')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngestMergeLoanTables():\n",
    "    def __init__(self, cf : dict, cf_m : dict) -> None:\n",
    "        \"\"\"\n",
    "        Class to ingest and merge all (bronze) tables in the loan dataset, to create a silver table with all merged data\n",
    "        \n",
    "        Args\n",
    "        ----------\n",
    "        cf : configuration file with constants used across project\n",
    "        cf_m : configuration file with constants used in this specific class\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        cf (dict) : same as cf input argument. Configuration file with constants used across projects\n",
    "        cf_m (dict) : same as cf_m input argument. Configuration file with constants used in this specific class\n",
    "        account (pd.DataFrame) : table containing account data. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.cf= cf\n",
    "        self.cf_m= cf_m\n",
    "\n",
    "        self.account= pd.DataFrame([])\n",
    "        self.card= pd.DataFrame([])\n",
    "        self.client= pd.DataFrame([])\n",
    "        self.disp= pd.DataFrame([])\n",
    "        self.district= pd.DataFrame([])\n",
    "        self.loan= pd.DataFrame([])\n",
    "        self.order= pd.DataFrame([])\n",
    "        self.transactions= pd.DataFrame([])\n",
    "        self.silver= pd.DataFrame([])\n",
    "\n",
    "        # add logger\n",
    "\n",
    "    def get_silver_table(self, schema : pa.DataFrameSchema):\n",
    "        \"\"\" \n",
    "        Wrapper method for retrieving the silver table containing all merged bronze tables, by either ingesting and merging all bronze tables \n",
    "        using ingest_bronze_tables(), cast_bronze_tables(), and merge_bronze_tables() methods \n",
    "        or by ingesting a previously saved silver table using load_from_disk() method\n",
    "\n",
    "        Option to save silver table to disk. Option available regardless if ingesting from scratch\n",
    "\n",
    "        Populates self.silver, returns self.silver\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        schema : expected schema of final silver dataframe (7 of 8 bronze tables merged into a single dataframe)\n",
    "        \"\"\"\n",
    "        if self.cf['FROM_SCRATCH']:\n",
    "            self.silver= self.ingest_bronze_tables().cast_bronze_dtypes().merge_bronze_tables()\n",
    "            # add validation \n",
    "        else:\n",
    "            self.silver= self.load_from_disk(schema= schema)\n",
    "            # add validation\n",
    "        \n",
    "        if self.cf['SAVE_SILVER']:\n",
    "            save_flat_file(path= self.cf['SILVER_SAVE_DIR'], filename= self.cf_m['bronze_merged_save_name'], df= self.silver)\n",
    "            # add log message\n",
    "\n",
    "        return self.silver\n",
    "\n",
    "    def ingest_bronze_tables(self):\n",
    "        \"\"\" \n",
    "        Ingest bronze tables, dropping nulls when adjusting using drop_nulls utility functions\n",
    "        Run validation to ensure all tables are what's expected\n",
    "\n",
    "        Populates self attributes account, card, client, disp, district, loan, order, and transactions\n",
    "        \"\"\"\n",
    "        self.account= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_account'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.card= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_card'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.client= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_client'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.disp= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_disp'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.district= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_district'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.loan= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_loan'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.order= drop_nulls(df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_order'], sep= self.cf_m['bronze_table_sep']))\n",
    "        self.transactions= drop_nulls(\n",
    "            df= pd.read_csv(self.cf['DATA_URL'] + self.cf_m['bronze_transactions'], sep= self.cf_m['bronze_table_sep']), \n",
    "            subset= self.cf_m['transactions_null_subset'])\n",
    "        \n",
    "        # add data validation on columns/dtypes\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def cast_bronze_dtypes(self):\n",
    "        \"\"\" \n",
    "        Method to cast all bronze tables to their required dtypes. Does not include district table. Intended to be run before merging all bronze tables\n",
    "        Modifies self attributes account, card, client, disp, loan, order, and transactions\n",
    "        \"\"\"\n",
    "        self.account= self.date_convert(df= self.account.astype(self.cf_m['account_dtypes']), date_col= self.cf_m['account_date_col'], date_format= self.cf_m['date_format'])\n",
    "        self.card= self.card.astype(self.cf_m['card_dtypes'])\n",
    "        self.client= self.date_convert(df= self.client.astype(self.cf_m['client_dtypes']), date_col= self.cf_m['client_date_col'], date_format= self.cf_m['date_format'])\n",
    "        self.disp= self.disp.astype(self.cf_m['disp_dtypes'])\n",
    "        self.loan= self.date_convert(df= self.loan.astype(self.cf_m['loan_dtypes']), date_col= self.cf_m['loan_date_col'], date_format= self.cf_m['date_format'])\n",
    "        self.order= self.order.astype(self.cf_m['order_dtypes'])\n",
    "        self.transactions= self.date_convert(df= self.transactions.astype(self.cf_m['transactions_dtypes']), date_col= self.cf_m['transactions_date_col'], date_format= self.cf_m['date_format'])\n",
    "\n",
    "        # add log message\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def date_convert(df : pd.DataFrame, date_col : str, date_format : str):\n",
    "        \"\"\" \n",
    "        Method for converting a str column in a pandas dataframe to a date column. Expected str format for the date column is '%y%m%d'\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dataframe containing the str column to be converted to dates\n",
    "        date_col : name of column in df with the str values to be converted to dates\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        df : same as input df but with all values in date_col converted to datetime.datetime\n",
    "\n",
    "        \"\"\"\n",
    "        df[date_col]= pd.to_datetime(df[date_col], format= date_format, errors= 'coerce')\n",
    "        return df\n",
    "\n",
    "    def merge_bronze_tables(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Method for merging all bronze tables into one silver table. Two merges are run in this method: first merges all dataframes with an \"account_id\"\n",
    "        column, second merges all dataframes with a \"client_id\" column. These two dataframes are then merged to create the silver dataframe\n",
    "        \n",
    "        Try/except blocks are used to provide informative log messages if a merge fails\n",
    "\n",
    "        Tables/dataframes being merged are self attributes account, loan, order, transactions, disp, client, and card\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        silver : dataframe with all bronze tables merged\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            account_id_merge= (self.account\n",
    "            .merge(self.loan, on= self.cf_m['account_id_merge_cols'], how = self.cf_m['account_id_merge_type'], suffixes= (self.cf_m['account_suffix'], self.cf_m['loan_suffix']))\n",
    "            .merge(self.order, on= self.cf_m['account_id_merge_cols'], how = self.cf_m['account_id_merge_type'], suffixes= (self.cf_m['loan_suffix'], self.cf_m['order_suffix']))\n",
    "            .merge(self.transactions, on= self.cf_m['account_id_merge_cols'], how = self.cf_m['account_id_merge_type'], suffixes= (self.cf_m['order_suffix'], self.cf_m['transactions_suffix']))\n",
    "            .merge(self.disp, on= self.cf_m['account_id_merge_cols'], how = self.cf_m['account_id_merge_type'], suffixes= (self.cf_m['transactions_suffix'], self.cf_m['disp_suffix']))\n",
    "            )\n",
    "        except:\n",
    "            raise(Exception)\n",
    "\n",
    "        try:\n",
    "            other_tables_merge= (self.disp\n",
    "            .merge(self.client, on= self.cf_m['client_id_merge_cols'], how= self.cf_m['client_id_merge_type'], suffixes= (self.cf_m['disp_suffix'], self.cf_m['client_suffix']))\n",
    "            .merge(self.card, on= self.cf_m['client_id_card_merge_cols'], how= self.cf_m['client_id_card_merge_type'], suffixes= (self.cf_m['client_disp_suffix'], self.cf_m['card_suffix']))\n",
    "            )\n",
    "        except:\n",
    "            raise(Exception)\n",
    "\n",
    "        try:\n",
    "            silver= other_tables_merge.merge(account_id_merge, on= self.cf_m['account_id_merge_cols'], how= self.cf_m['account_id_merge_type'], suffixes= ('', self.cf_m['disp_id_suffix']))\n",
    "            # log message\n",
    "        except:\n",
    "            raise(Exception)\n",
    "        return silver\n",
    "    \n",
    "    def load_from_disk(self, schema : pa.DataFrameSchema):\n",
    "        silver, msg= ingest_flat_file(path= self.cf['DATA_URL'], filename= self.cf['BRONZE_MERGED_FILE'], schema= schema)\n",
    "        # add log message\n",
    "        return silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\valerie.bauman\\Documents\\GitHub\\sample-work-loan-application\\ingestion_config.yml', 'r') as stream:\n",
    "    cf= yaml.safe_load(stream)\n",
    "    \n",
    "bronze_loader= IngestMergeLoanTables(cf= cf, cf_m= cf['ingest'])\n",
    "bronze_merged= bronze_loader.get_silver_table(schema= bronze_merge_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaError",
     "evalue": "expected series 'amount_loan' to have type int64, got int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VALERI~1.BAU\\AppData\\Local\\Temp/ipykernel_24096/1685823869.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbronze_merge_schema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbronze_merged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schemas.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandera\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         return self._validate(\n\u001b[0m\u001b[0;32m    535\u001b[0m             \u001b[0mcheck_obj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mhead\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schemas.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[0;32m    730\u001b[0m                 \u001b[0mcheck_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSchemaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                 \u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"schema_component_check\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSchemaErrors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mschema_error_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema_errors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\error_handlers.py\u001b[0m in \u001b[0;36mcollect_error\u001b[1;34m(self, reason_code, schema_error, original_exc)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \"\"\"\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mschema_error\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moriginal_exc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# delete data of validated object from SchemaError object to prevent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schemas.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mschema_component\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mschema_components\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m                 result = schema_component(\n\u001b[0m\u001b[0;32m    725\u001b[0m                     \u001b[0mdf_to_validate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m                     \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schemas.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[0;32m   2136\u001b[0m     ) -> Union[pd.DataFrame, pd.Series]:\n\u001b[0;32m   2137\u001b[0m         \u001b[1;34m\"\"\"Alias for ``validate`` method.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2138\u001b[1;33m         return self.validate(\n\u001b[0m\u001b[0;32m   2139\u001b[0m             \u001b[0mcheck_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2140\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schema_components.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[0;32m    221\u001b[0m                     )\n\u001b[0;32m    222\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mvalidate_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schema_components.py\u001b[0m in \u001b[0;36mvalidate_column\u001b[1;34m(check_obj, column_name)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mvalidate_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             super(Column, copy(self).set_name(column_name)).validate(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mcheck_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\schemas.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[0;32m   2069\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfailure_cases\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfailure_cases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2071\u001b[1;33m                 error_handler.collect_error(\n\u001b[0m\u001b[0;32m   2072\u001b[0m                     \u001b[1;34m\"wrong_dtype\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2073\u001b[0m                     errors.SchemaError(\n",
      "\u001b[1;32mc:\\Users\\valerie.bauman\\.venv\\lib\\site-packages\\pandera\\error_handlers.py\u001b[0m in \u001b[0;36mcollect_error\u001b[1;34m(self, reason_code, schema_error, original_exc)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \"\"\"\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mschema_error\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moriginal_exc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# delete data of validated object from SchemaError object to prevent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSchemaError\u001b[0m: expected series 'amount_loan' to have type int64, got int32"
     ]
    }
   ],
   "source": [
    "bronze_merge_schema.validate(bronze_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hl.m. Praha</td>\n",
       "      <td>Prague</td>\n",
       "      <td>1204953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12541</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>167</td>\n",
       "      <td>85677</td>\n",
       "      <td>99107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Benesov</td>\n",
       "      <td>central Bohemia</td>\n",
       "      <td>88884</td>\n",
       "      <td>80</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>46.7</td>\n",
       "      <td>8507</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.85</td>\n",
       "      <td>132</td>\n",
       "      <td>2159</td>\n",
       "      <td>2674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Beroun</td>\n",
       "      <td>central Bohemia</td>\n",
       "      <td>75232</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>41.7</td>\n",
       "      <td>8980</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.21</td>\n",
       "      <td>111</td>\n",
       "      <td>2824</td>\n",
       "      <td>2813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kladno</td>\n",
       "      <td>central Bohemia</td>\n",
       "      <td>149893</td>\n",
       "      <td>63</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>67.4</td>\n",
       "      <td>9753</td>\n",
       "      <td>4.64</td>\n",
       "      <td>5.05</td>\n",
       "      <td>109</td>\n",
       "      <td>5244</td>\n",
       "      <td>5892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Kolin</td>\n",
       "      <td>central Bohemia</td>\n",
       "      <td>95616</td>\n",
       "      <td>65</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>51.4</td>\n",
       "      <td>9307</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.43</td>\n",
       "      <td>118</td>\n",
       "      <td>2616</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1           A2               A3       A4  A5  A6  A7  A8  A9    A10  \\\n",
       "0   1  Hl.m. Praha           Prague  1204953   0   0   0   1   1  100.0   \n",
       "1   2      Benesov  central Bohemia    88884  80  26   6   2   5   46.7   \n",
       "2   3       Beroun  central Bohemia    75232  55  26   4   1   5   41.7   \n",
       "3   4       Kladno  central Bohemia   149893  63  29   6   2   6   67.4   \n",
       "4   5        Kolin  central Bohemia    95616  65  30   4   1   6   51.4   \n",
       "\n",
       "     A11   A12   A13  A14    A15    A16  \n",
       "0  12541  0.29  0.43  167  85677  99107  \n",
       "1   8507  1.67  1.85  132   2159   2674  \n",
       "2   8980  1.95  2.21  111   2824   2813  \n",
       "3   9753  4.64  5.05  109   5244   5892  \n",
       "4   9307  3.85  4.43  118   2616   3040  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final goal is to predict loan outcome for finished loans at the time of loan start\n",
    "# in status column in loan, A is good loan that is finished, B is bad loan that is finished, \n",
    "# C is good loan that is unfinished, D is bad loan that is unfinished\n",
    "district.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A', 'B', 'C', 'D'], dtype=object),\n",
       " array([203,  31, 403,  45], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(loan.status.values, return_counts= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0dbcac84d6f087bec5c68a30215510514632c640c39b308fbd052d5a40b318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
